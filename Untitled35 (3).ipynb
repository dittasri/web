{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e9f769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Headers\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you knowÂ ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape header tags from a given URL\n",
    "def scrape_headers(url):\n",
    "    headers = []\n",
    "\n",
    "    # Make an HTTP request\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all header tags (h1 to h6)\n",
    "        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            headers.append(tag.text.strip())\n",
    "\n",
    "    return headers\n",
    "\n",
    "# URL of the Wikipedia page\n",
    "wiki_url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "\n",
    "# Scrape header tags\n",
    "header_tags = scrape_headers(wiki_url)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Headers': header_tags})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c70041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Term of Office]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape former presidents from the given URL\n",
    "def scrape_former_presidents(url):\n",
    "    presidents_data = {'Name': [], 'Term of Office': []}\n",
    "\n",
    "    # Make an HTTP request\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the table containing the former presidents' information\n",
    "        table = soup.find('table', {'class': 'tablepress'})\n",
    "        \n",
    "        # Find all rows in the table (skipping the header row)\n",
    "        rows = table.find_all('tr')[1:]\n",
    "\n",
    "        for row in rows:\n",
    "            # Extract the name and term of office from each row\n",
    "            columns = row.find_all('td')\n",
    "            name = columns[0].text.strip()\n",
    "            term_of_office = columns[1].text.strip()\n",
    "\n",
    "            # Append data to the dictionary\n",
    "            presidents_data['Name'].append(name)\n",
    "            presidents_data['Term of Office'].append(term_of_office)\n",
    "\n",
    "    return presidents_data\n",
    "\n",
    "# URL of the page with former presidents' information\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "\n",
    "# Scrape former presidents\n",
    "former_presidents_data = scrape_former_presidents(url)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(former_presidents_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec31609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0        India\\nIND      55  6,640    121\n",
      "1    Australia\\nAUS      42  4,926    117\n",
      "2  South Africa\\nSA      34  3,750    110\n",
      "3     Pakistan\\nPAK      36  3,922    109\n",
      "4   New Zealand\\nNZ      43  4,399    102\n",
      "5      England\\nENG      38  3,777     99\n",
      "6     Sri Lanka\\nSL      47  4,134     88\n",
      "7   Bangladesh\\nBAN      44  3,836     87\n",
      "8  Afghanistan\\nAFG      30  2,533     84\n",
      "9   West Indies\\nWI      38  2,582     68\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "team_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  team = cells[1].text.strip()\n",
    "  matches = cells[2].text.strip()\n",
    "  points = cells[3].text.strip()\n",
    "  rating = cells[4].text.strip()\n",
    "  team_data.append([team, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(team_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb7f73a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Batsman Team Rating\n",
      "0           Shubman Gill  IND    826\n",
      "1             Babar Azam  PAK    824\n",
      "2            Virat Kohli  IND    791\n",
      "3           Rohit Sharma  IND    769\n",
      "4        Quinton de Kock   SA    760\n",
      "5         Daryl Mitchell   NZ    750\n",
      "6           David Warner  AUS    745\n",
      "7  Rassie van der Dussen   SA    735\n",
      "8           Harry Tector  IRE    729\n",
      "9            Dawid Malan  ENG    729\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsman_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  batsman = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  batsman_data.append([batsman, team, rating])\n",
    "\n",
    "df = pd.DataFrame(batsman_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bba39938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Bowler Team Rating\n",
      "0  Keshav Maharaj   SA    741\n",
      "1  Josh Hazlewood  AUS    703\n",
      "2  Mohammed Siraj  IND    699\n",
      "3  Jasprit Bumrah  IND    685\n",
      "4      Adam Zampa  AUS    675\n",
      "5     Rashid Khan  AFG    667\n",
      "6   Kuldeep Yadav  IND    667\n",
      "7     Trent Boult   NZ    663\n",
      "8  Shaheen Afridi  PAK    650\n",
      "9  Mohammad Shami  IND    648\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "bowler_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  bowler = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  bowler_data.append([bowler, team, rating])\n",
    "\n",
    "df = pd.DataFrame(bowler_data, columns=[\"Bowler\", \"Team\", \"Rating\"])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81ad806a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      21  3,429    163\n",
      "1      England\\nENG      23  2,991    130\n",
      "2  South Africa\\nSA      21  2,446    116\n",
      "3        India\\nIND      18  1,745     97\n",
      "4   New Zealand\\nNZ      21  2,014     96\n",
      "5   West Indies\\nWI      20  1,768     88\n",
      "6     Sri Lanka\\nSL       9    714     79\n",
      "7   Bangladesh\\nBAN      14  1,074     77\n",
      "8     Thailand\\nTHA      11    753     68\n",
      "9     Pakistan\\nPAK      24  1,602     67\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "team_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  team = cells[1].text.strip()\n",
    "  matches = cells[2].text.strip()\n",
    "  points = cells[3].text.strip()\n",
    "  rating = cells[4].text.strip()\n",
    "  team_data.append([team, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(team_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "983e1161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Batsman Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    807\n",
      "1           Beth Mooney  AUS    750\n",
      "2   Chamari Athapaththu   SL    736\n",
      "3       Laura Wolvaardt   SA    727\n",
      "4       Smriti Mandhana  IND    708\n",
      "5          Alyssa Healy  AUS    698\n",
      "6          Ellyse Perry  AUS    697\n",
      "7      Harmanpreet Kaur  IND    694\n",
      "8           Meg Lanning  AUS    662\n",
      "9        Marizanne Kapp   SA    642\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsman_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  batsman = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  batsman_data.append([batsman, team, rating])\n",
    "\n",
    "df = pd.DataFrame(batsman_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "136da96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Bowler Team Rating\n",
      "0  Sophie Ecclestone  ENG    746\n",
      "1     Shabnim Ismail   SA    680\n",
      "2      Jess Jonassen  AUS    662\n",
      "3       Megan Schutt  AUS    658\n",
      "4   Ashleigh Gardner  AUS    652\n",
      "5     Ayabonga Khaka   SA    640\n",
      "6         Kate Cross  ENG    628\n",
      "7    Hayley Matthews   WI    625\n",
      "8      Deepti Sharma  IND    607\n",
      "9     Marizanne Kapp   SA    600\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "bowler_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  bowler = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  bowler_data.append([bowler, team, rating])\n",
    "\n",
    "df = pd.DataFrame(bowler_data, columns=[\"Bowler\", \"Team\", \"Rating\"])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dff09ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Gaza truce enters fifth day with more releases due\n",
      "Toxic gas putting millions at risk in Middle East\n",
      "41 men trapped for 16 days: India's stark wake-up call\n",
      "TikTok owner ByteDance cuts gaming division jobs\n",
      "India actress urges women to speak up on deepfake\n",
      "'Gran, I've been shot,' said US-Palestinian student\n",
      "Australia to ban disposable vape imports from January\n",
      "First green-fuelled transatlantic flight to take off\n",
      "Ninety-eight trees and a gingerbread White House\n",
      "Niger coup leaders repeal law against migrant smuggling\n",
      "New book promises to detail Royal turmoil\n",
      "Niger coup leaders repeal law against migrant smuggling\n",
      "New book promises to detail Royal turmoil\n",
      "UK PM Sunak cancels Greek PM meeting in Parthenon Sculptures row\n",
      "Imran Khan's ex-adviser attacked with acid in UK\n",
      "Elon Musk visits Israel after antisemitism row\n",
      "Three-year-old-twins among hostages released by Hamas\n",
      "Israel frees 33 Palestinians on fourth day of truce\n",
      "Israel-Hamas truce extended for two days - Qatar\n",
      "Hostage 'escaped Hamas and hid in Gaza for days'\n",
      "Watch: Gaza families living in tents with no winter clothing\n",
      "From bidding for Bale to selling the team bus - the fall of the CSL\n",
      "BBC World News TV\n",
      "BBC World Service Radio\n",
      "How Hamas built a force to attack Israel on 7 October\n",
      "The T-shirt chewing enzyme ready to tackle plastic waste\n",
      "The Obama-backed film that could win its star an Oscar\n",
      "The English town with a curious Canadian corner\n",
      "What may have sparked the gunfight in Sierra Leone\n",
      "Shock as New Zealand axes world-first smoking ban\n",
      "What's Merriam-Webster's new word of the year?\n",
      "Thai groom kills four at wedding, including bride\n",
      "Watch: Huge waves hit Russian coast in Winter storms\n",
      "Bird flu kills hundreds of flamingos in Argentina\n",
      "Venables the best English coach we've had - Lineker\n",
      "Lightning and hailstorms kill 24 in western India\n",
      "The world's most powerful lasers\n",
      "How EcoVadis holds firms accountable\n",
      "Why Swedes donât like chit-chat\n",
      "A guide to giving gifts sustainably\n",
      "The phone you can repair yourself\n",
      "Travellers are snubbing budget airlines\n",
      "The tunnels made by giant sloths\n",
      "News daily newsletter\n",
      "Mobile app\n",
      "Get in touch\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url='https://www.bbc.com/news'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "headlines = soup.find('body').find_all('h3')\n",
    "for x in headlines:\n",
    "    print(x.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23950453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape details from the Elsevier journal page\n",
    "def scrape_elsevier_journal(url):\n",
    "    data = {'Paper Title': [], 'Authors': [], 'Published Date': [], 'Paper URL': []}\n",
    "\n",
    "    # Make an HTTP request\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the container with article details\n",
    "        articles = soup.find_all('div', class_='pod-listing')\n",
    "\n",
    "        for article in articles:\n",
    "            # Extract data from each article\n",
    "            title = article.find('a', class_='pod-listing-header').text.strip()\n",
    "            authors = article.find('div', class_='js-authors-container').text.strip()\n",
    "            date = article.find('div', class_='pod-listing-meta').text.strip()\n",
    "            paper_url = 'https://www.journals.elsevier.com' + article.find('a', class_='pod-listing-header')['href']\n",
    "\n",
    "            # Append data to the dictionary\n",
    "            data['Paper Title'].append(title)\n",
    "            data['Authors'].append(authors)\n",
    "            data['Published Date'].append(date)\n",
    "            data['Paper URL'].append(paper_url)\n",
    "\n",
    "    return data\n",
    "\n",
    "# URL for the Elsevier journal most downloaded articles\n",
    "elsevier_url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "\n",
    "# Scrape article details\n",
    "elsevier_data = scrape_elsevier_journal(elsevier_url)\n",
    "\n",
    "# Create DataFrame\n",
    "elsevier_df = pd.DataFrame(elsevier_data)\n",
    "\n",
    "# Display DataFrame\n",
    "print(elsevier_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
